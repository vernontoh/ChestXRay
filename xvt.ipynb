{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CvtConfig, CvtModel\n",
    "\n",
    "# Initializing a Cvt msft/cvt style configuration\n",
    "configuration = CvtConfig()\n",
    "\n",
    "# Initializing a model (with random weights) from the msft/cvt style configuration\n",
    "model = CvtModel(configuration)\n",
    "\n",
    "# Accessing the model configuration\n",
    "configuration = model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CvtConfig {\n",
       "  \"attention_drop_rate\": [\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0\n",
       "  ],\n",
       "  \"cls_token\": [\n",
       "    false,\n",
       "    false,\n",
       "    true\n",
       "  ],\n",
       "  \"depth\": [\n",
       "    1,\n",
       "    2,\n",
       "    10\n",
       "  ],\n",
       "  \"drop_path_rate\": [\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.1\n",
       "  ],\n",
       "  \"drop_rate\": [\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0\n",
       "  ],\n",
       "  \"embed_dim\": [\n",
       "    64,\n",
       "    192,\n",
       "    384\n",
       "  ],\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"kernel_qkv\": [\n",
       "    3,\n",
       "    3,\n",
       "    3\n",
       "  ],\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"mlp_ratio\": [\n",
       "    4.0,\n",
       "    4.0,\n",
       "    4.0\n",
       "  ],\n",
       "  \"model_type\": \"cvt\",\n",
       "  \"num_channels\": 3,\n",
       "  \"num_heads\": [\n",
       "    1,\n",
       "    3,\n",
       "    6\n",
       "  ],\n",
       "  \"padding_kv\": [\n",
       "    1,\n",
       "    1,\n",
       "    1\n",
       "  ],\n",
       "  \"padding_q\": [\n",
       "    1,\n",
       "    1,\n",
       "    1\n",
       "  ],\n",
       "  \"patch_padding\": [\n",
       "    2,\n",
       "    1,\n",
       "    1\n",
       "  ],\n",
       "  \"patch_sizes\": [\n",
       "    7,\n",
       "    3,\n",
       "    3\n",
       "  ],\n",
       "  \"patch_stride\": [\n",
       "    4,\n",
       "    2,\n",
       "    2\n",
       "  ],\n",
       "  \"qkv_bias\": [\n",
       "    true,\n",
       "    true,\n",
       "    true\n",
       "  ],\n",
       "  \"qkv_projection_method\": [\n",
       "    \"dw_bn\",\n",
       "    \"dw_bn\",\n",
       "    \"dw_bn\"\n",
       "  ],\n",
       "  \"stride_kv\": [\n",
       "    2,\n",
       "    2,\n",
       "    2\n",
       "  ],\n",
       "  \"stride_q\": [\n",
       "    1,\n",
       "    1,\n",
       "    1\n",
       "  ],\n",
       "  \"transformers_version\": \"4.26.0\"\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Some weights of the model checkpoint at microsoft/cvt-13 were not used when initializing CvtModel: ['layernorm.bias', 'classifier.bias', 'classifier.weight', 'layernorm.weight']\n",
      "- This IS expected if you are initializing CvtModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CvtModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 384, 14, 14]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, CvtModel\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "image = Image.open(\"ChestX-ray14/images/00000001_000.png\").convert('RGB')\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"microsoft/cvt-13\")\n",
    "model = CvtModel.from_pretrained(\"microsoft/cvt-13\")\n",
    "\n",
    "inputs = image_processor(image, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "list(last_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"pixel_values\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, CvtForImageClassification\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "image = Image.open(\"ChestX-ray14/images/00000001_000.png\").convert('RGB')\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"microsoft/cvt-13\")\n",
    "model = CvtForImageClassification.from_pretrained(\"microsoft/cvt-13\")\n",
    "\n",
    "inputs = image_processor(image, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "# predicted_label = logits.argmax(-1).item()\n",
    "# print(model.config.id2label[predicted_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─CvtModel: 1-1                          [[-1, 1, 384]]            --\n",
      "|    └─CvtEncoder: 2-1                   [[-1, 1, 384]]            --\n",
      "├─LayerNorm: 1-2                         [-1, 1, 384]              768\n",
      "├─Linear: 1-3                            [-1, 1000]                385,000\n",
      "==========================================================================================\n",
      "Total params: 385,768\n",
      "Trainable params: 385,768\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 59.03\n",
      "==========================================================================================\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 1.47\n",
      "Estimated Total Size (MB): 2.06\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─CvtModel: 1-1                          [[-1, 1, 384]]            --\n",
       "|    └─CvtEncoder: 2-1                   [[-1, 1, 384]]            --\n",
       "├─LayerNorm: 1-2                         [-1, 1, 384]              768\n",
       "├─Linear: 1-3                            [-1, 1000]                385,000\n",
       "==========================================================================================\n",
       "Total params: 385,768\n",
       "Trainable params: 385,768\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 59.03\n",
       "==========================================================================================\n",
       "Input size (MB): 0.57\n",
       "Forward/backward pass size (MB): 0.01\n",
       "Params size (MB): 1.47\n",
       "Estimated Total Size (MB): 2.06\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(4, 4)\n",
    "x.size()\n",
    "y = x.view(16)\n",
    "y.size()\n",
    "z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
    "z.size()\n",
    "\n",
    "a = torch.randn(1, 2, 3, 4)\n",
    "a.size()\n",
    "b = a.transpose(1, 2)  # Swaps 2nd and 3rd dimension\n",
    "b.size()\n",
    "c = a.view(1, 3, 2, 4)  # Does not change tensor layout in memory\n",
    "c.size()\n",
    "torch.equal(b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 784, 3])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(128, 3, 28, 28)\n",
    "\n",
    "x1 = x.view(128, 3, 784).permute(0, 2, 1)\n",
    "\n",
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m bs, hs, c \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "bs, hs, c = x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 784, 3])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 3, 28, 28])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.permute(0,2,1).view(128, 3, 28, 28).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "layernorm = nn.LayerNorm(382)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_output shape: torch.Size([128, 382, 28, 28])\n",
      "encoder_output shape: torch.Size([128, 784, 382])\n",
      "encoder_output shape: torch.Size([128, 784, 382])\n",
      "encoder_output_mean shape: torch.Size([128, 382])\n"
     ]
    }
   ],
   "source": [
    "encoder_output = torch.randn(128, 382, 28, 28)\n",
    "\n",
    "batch_size, num_channels, height, width = encoder_output.shape\n",
    "print(f\"encoder_output shape: {encoder_output.shape}\")\n",
    "\n",
    "# rearrange \"b c h w -> b (h w) c\"\n",
    "encoder_output = encoder_output.view(batch_size, num_channels, height * width).permute(0, 2, 1)\n",
    "print(f\"encoder_output shape: {encoder_output.shape}\")\n",
    "\n",
    "encoder_output = layernorm(encoder_output)\n",
    "print(f\"encoder_output shape: {encoder_output.shape}\")\n",
    "\n",
    "\n",
    "encoder_output_mean = encoder_output.mean(dim=1)\n",
    "print(f\"encoder_output_mean shape: {encoder_output_mean.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modeling_xvt\n",
    "import modeling_lightweight_xvt\n",
    "import configuration_xvt\n",
    "import importlib\n",
    "# importlib.reload(modeling_xvt)\n",
    "importlib.reload(modeling_lightweight_xvt)\n",
    "importlib.reload(configuration_xvt)\n",
    "\n",
    "# from modeling_xvt import XvtForImageClassification\n",
    "from modeling_lightweight_xvt import XvtForImageClassification\n",
    "from configuration_xvt import XvtConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = XvtConfig()\n",
    "model = XvtForImageClassification(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(pixel_values=pixel_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #\n",
      "===============================================================================================\n",
      "├─XvtConvEmbeddings: 1-1                      [-1, 32, 56, 56]          --\n",
      "|    └─Conv2d: 2-1                            [-1, 32, 56, 56]          4,736\n",
      "|    └─LayerNorm: 2-2                         [-1, 3136, 32]            64\n",
      "├─XvtEncoder: 1-2                             [-1, 32, 56, 56]          --\n",
      "|    └─Sequential: 2                          []                        --\n",
      "|    |    └─XvtLayer: 3-1                     [-1, 3136, 32]            13,760\n",
      "├─LayerNorm: 1-3                              [-1, 3136, 32]            64\n",
      "├─Linear: 1-4                                 [-1, 14]                  462\n",
      "===============================================================================================\n",
      "Total params: 19,086\n",
      "Trainable params: 19,086\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 14.80\n",
      "===============================================================================================\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 3.83\n",
      "Params size (MB): 0.07\n",
      "Estimated Total Size (MB): 4.48\n",
      "===============================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "├─XvtConvEmbeddings: 1-1                      [-1, 32, 56, 56]          --\n",
       "|    └─Conv2d: 2-1                            [-1, 32, 56, 56]          4,736\n",
       "|    └─LayerNorm: 2-2                         [-1, 3136, 32]            64\n",
       "├─XvtEncoder: 1-2                             [-1, 32, 56, 56]          --\n",
       "|    └─Sequential: 2                          []                        --\n",
       "|    |    └─XvtLayer: 3-1                     [-1, 3136, 32]            13,760\n",
       "├─LayerNorm: 1-3                              [-1, 3136, 32]            64\n",
       "├─Linear: 1-4                                 [-1, 14]                  462\n",
       "===============================================================================================\n",
       "Total params: 19,086\n",
       "Trainable params: 19,086\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 14.80\n",
       "===============================================================================================\n",
       "Input size (MB): 0.57\n",
       "Forward/backward pass size (MB): 3.83\n",
       "Params size (MB): 0.07\n",
       "Estimated Total Size (MB): 4.48\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 30])\n"
     ]
    }
   ],
   "source": [
    "m = nn.Linear(20, 30)\n",
    "input = torch.randn(1, 128, 20)\n",
    "output = m(input)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\"one\": 1, \"two\":2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'two'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[156], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m args\u001b[39m.\u001b[39;49mtwo\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'two'"
     ]
    }
   ],
   "source": [
    "args.two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'XvtScheduler' from 'configuration_xvt' (/Users/vernontoh/SUTD/Term6/Theory and Practice of Deep Learning/ChestXRay/configuration_xvt.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[157], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mconfiguration_xvt\u001b[39;00m \u001b[39mimport\u001b[39;00m XvtScheduler\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'XvtScheduler' from 'configuration_xvt' (/Users/vernontoh/SUTD/Term6/Theory and Practice of Deep Learning/ChestXRay/configuration_xvt.py)"
     ]
    }
   ],
   "source": [
    "from configuration_xvt import XvtScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
